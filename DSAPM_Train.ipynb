{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOK4plVFxMZTW1yi3ZdPhaK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Dual Stage Attention Prediction Module**"],"metadata":{"id":"5-BYFv5vH_TY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQsXE3Cvmcs9"},"outputs":[],"source":["# Initialization\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","import glob\n","import os\n","import pandas as pd\n","from google.colab.patches import cv2_imshow\n","from tensorflow.keras import layers, models, losses, optimizers\n","from math import log10, sqrt\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Load UCSD Ped1 Dataset (7200 images)\n","all_images = []\n","im_dir = \"/content/drive/MyDrive/Train_Data\"\n","list_of_files = sorted(os.listdir(im_dir), reverse=False)\n","\n","for im_folder in list_of_files:  # iterate over each folder\n","    list_of_img_files = sorted(os.listdir(im_dir + '/' + im_folder), reverse=False)\n","    for image_file in list_of_img_files:  # iterate over each image\n","        image = cv2.imread((im_dir + '/' + im_folder + '/' + image_file), cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n","        image = cv2.resize(image, (128, 128))  # Resize to 128x128\n","        all_images.append(image)\n","\n","# Convert to NumPy array and reshape\n","all_images = np.array(all_images).reshape(340, 20, 128, 128, 1) # shape for 7200 images\n","\n","# Split into training and validation sets\n","indexes = np.arange(all_images.shape[0])\n","np.random.shuffle(indexes)\n","train_index = indexes[: int(0.9 * all_images.shape[0])]\n","val_index = indexes[int(0.9 * all_images.shape[0]):]\n","\n","train_dataset = all_images[train_index] / 255.0  # Normalize\n","val_dataset = all_images[val_index] / 255.0  # Normalize\n","\n","# Helper function to create shifted frames\n","def create_shifted_frames(data):\n","    x = data[:, :-1, :, :, :]\n","    y = data[:, 1:, :, :, :]\n","    return x, y\n","\n","# Apply frame shifting\n","x_train, y_train = create_shifted_frames(train_dataset)\n","x_val, y_val = create_shifted_frames(val_dataset)\n","\n","# Inspect the dataset\n","print(\"Training Dataset Shapes:\", x_train.shape, y_train.shape)\n","print(\"Validation Dataset Shapes:\", x_val.shape, y_val.shape)\n","\n","# Visualizing sample frames\n","fig, axes = plt.subplots(4, 5, figsize=(10, 8))\n","data_choice = np.random.choice(range(len(train_dataset)), size=1)[0]\n","\n","for idx, ax in enumerate(axes.flat):\n","    ax.imshow(np.squeeze(train_dataset[data_choice][idx]), cmap=\"gray\")\n","    ax.set_title(f\"Frame {idx + 1}\")\n","    ax.axis(\"off\")\n","plt.show()\n","\n","# CBAM Attention Block\n","import tensorflow.keras.backend as K\n","\n","def cbam_block(input_tensor, reduction_ratio=8):\n","    # Channel Attention\n","    channel_avg_pool = layers.GlobalAveragePooling3D()(input_tensor)\n","    channel_max_pool = layers.GlobalMaxPooling3D()(input_tensor)\n","\n","    channel_avg_pool = layers.Dense(units=input_tensor.shape[-1] // reduction_ratio, activation=\"relu\")(channel_avg_pool)\n","    channel_avg_pool = layers.Dense(units=input_tensor.shape[-1], activation=\"sigmoid\")(channel_avg_pool)\n","\n","    channel_max_pool = layers.Dense(units=input_tensor.shape[-1] // reduction_ratio, activation=\"relu\")(channel_max_pool)\n","    channel_max_pool = layers.Dense(units=input_tensor.shape[-1], activation=\"sigmoid\")(channel_max_pool)\n","\n","    channel_attention = layers.Add()([channel_avg_pool, channel_max_pool])\n","    channel_attention = layers.Multiply()([input_tensor, channel_attention])\n","\n","    # Spatial Attention\n","    spatial_avg_pool = layers.Lambda(lambda x: K.expand_dims(K.mean(x, axis=-1), axis=-1))(channel_attention)\n","    spatial_max_pool = layers.Lambda(lambda x: K.expand_dims(K.max(x, axis=-1), axis=-1))(channel_attention)\n","    spatial_attention = layers.Concatenate(axis=-1)([spatial_avg_pool, spatial_max_pool])\n","    spatial_attention = layers.Conv3D(filters=1, kernel_size=7, activation=\"sigmoid\", padding=\"same\")(spatial_attention)\n","    spatial_attention = layers.Multiply()([channel_attention, spatial_attention])\n","\n","    return spatial_attention\n","\n","# DSAPM Model\n","inp = layers.Input(shape=(None, *x_train.shape[2:]))\n","\n","# Block 1\n","x1 = layers.ConvLSTM2D(filters=64, kernel_size=(5, 5), padding=\"same\", return_sequences=True, activation=\"relu\")(inp)\n","x1 = layers.BatchNormalization()(x1)\n","x1 = cbam_block(x1)\n","\n","# Block 2\n","x2 = layers.ConvLSTM2D(filters=64, kernel_size=(5, 5), padding=\"same\", return_sequences=True, activation=\"relu\")(x1)\n","x2 = layers.BatchNormalization()(x2)\n","x2 = cbam_block(x2)\n","x2 = layers.Add()([x1, x2])\n","\n","# Block 3\n","x3 = layers.ConvLSTM2D(filters=128, kernel_size=(3, 3), padding=\"same\", return_sequences=True, activation=\"relu\")(x2)\n","x3 = layers.BatchNormalization()(x3)\n","x3 = cbam_block(x3)\n","x3 = layers.Add()([x2, x3])\n","\n","# Block 4\n","x4 = layers.ConvLSTM2D(filters=128, kernel_size=(3, 3), padding=\"same\", return_sequences=True, activation=\"relu\")(x3)\n","x4 = layers.BatchNormalization()(x4)\n","x4 = cbam_block(x4)\n","x4 = layers.Add()([x3, x4])\n","\n","# Block 5\n","x5 = layers.ConvLSTM2D(filters=64, kernel_size=(1, 1), padding=\"same\", return_sequences=True, activation=\"relu\")(x4)\n","x5 = layers.BatchNormalization()(x5)\n","x5 = cbam_block(x5)\n","x5 = layers.Add()([x4, x5])\n","\n","# Output Layer\n","x = layers.Conv3D(filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\")(x5)\n","\n","# Compile Model\n","model = keras.models.Model(inp, x)\n","model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam())\n","model.summary()\n","\n","# Training\n","early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n","\n","model.fit(\n","    x_train, y_train,\n","    batch_size=64,\n","    epochs=40,\n","    validation_data=(x_val, y_val),\n","    callbacks=[early_stopping, reduce_lr],\n",")\n","\n","# Calculate MSE\n","MSE = []\n","for j in range(len(train_dataset)):\n","    example = train_dataset[j]\n","    frames = example[:19, :, :, :]\n","    original_frame = example[19, :, :, :]\n","\n","    predicted_frames = model.predict(np.expand_dims(frames, axis=0))\n","    predicted_frame = np.squeeze(predicted_frames)[-1, :, :, :]\n","\n","    mse_value = np.square(np.subtract(predicted_frame, original_frame)).mean()\n","    MSE.append(mse_value)\n","\n","print(\"MSE List:\", MSE)\n","\n","# Calculate PSNR\n","PSNR = []\n","max_pixel = 1.0  # Normalized pixel range\n","\n","for mse_value in MSE:\n","    if mse_value == 0:\n","        psnr_value = float('inf')  # Avoid division by zero\n","    else:\n","        psnr_value = 20 * log10(max_pixel / sqrt(mse_value))\n","    PSNR.append(psnr_value)\n","\n","print(\"PSNR List:\", PSNR)\n","\n","# Normalize PSNR\n","scaler = MinMaxScaler()\n","norm_PSNR_predicted = scaler.fit_transform(np.array(PSNR).reshape(-1, 1)).flatten()\n","\n","# Compute Frame Scores\n","Predicted_train = norm_PSNR_predicted.tolist()\n","\n","# Save to Excel\n","df = pd.DataFrame({\"Frame Score\": Predicted_train})\n","df.to_excel(\"frame_scores_train.xlsx\", index=False)\n","\n","# Save Model\n","os.makedirs(\"models\", exist_ok=True)\n","model.save(\"models/dsapm.h5\")\n"]}]}