{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation_Metrics** This code first extract the ground truth labels from all the four datasets and then computes the confusion matrix, plots ROC, computes AUC, Precision, Recall, F1-score and Specificity"
      ],
      "metadata": {
        "id": "h0ixQKumvpCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "\n",
        "# Define paths for all dataset ground truth files\n",
        "gt_files = {\n",
        "    \"UCSD_Ped1\": \"UCSD_Ped1/TestAnnotation.mat\",\n",
        "    \"UCSD_Ped2\": \"UCSD_Ped2/TestAnnotation.mat\",\n",
        "    \"Avenue\": \"Avenue/Test.txt\",\n",
        "    \"ShanghaiTech\": \"ShanghaiTech/test_frame_mask/\"\n",
        "}\n",
        "\n",
        "# Initialize dictionary to store labels\n",
        "gt_data_dict = {}\n",
        "\n",
        "# Process UCSD Ped1 and Ped2\n",
        "for dataset, file_path in gt_files.items():\n",
        "    if \"UCSD\" in dataset:\n",
        "        gt_data = scipy.io.loadmat(file_path)\n",
        "        gt_frames = gt_data[\"gt\"]\n",
        "\n",
        "        num_test_videos = len(gt_frames)\n",
        "        actual_labels = []\n",
        "\n",
        "        for i in range(num_test_videos):\n",
        "            labels = np.zeros(200)  # 200 frames per video\n",
        "            for anomaly_range in gt_frames[i]:\n",
        "                start, end = anomaly_range[0], anomaly_range[1]\n",
        "                labels[start:end + 1] = 1  # Mark anomalies\n",
        "            actual_labels.append(labels)\n",
        "\n",
        "        actual_labels = np.concatenate(actual_labels)  # Flatten\n",
        "        gt_data_dict[dataset] = actual_labels  # Store labels\n",
        "\n",
        "# Process Avenue dataset\n",
        "with open(gt_files[\"Avenue\"], \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "actual_labels = np.zeros(10000)  # 10,000 frames per video\n",
        "frame_idx = 0\n",
        "\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    if line == \"-1\":  # End of a video's annotations\n",
        "        frame_idx += 1\n",
        "        continue\n",
        "    start, end = map(int, line.split())\n",
        "    actual_labels[start:end + 1] = 1  # Mark anomalies\n",
        "\n",
        "actual_labels = actual_labels[:frame_idx]\n",
        "gt_data_dict[\"Avenue\"] = actual_labels  # Store labels\n",
        "\n",
        "# Process ShanghaiTech dataset\n",
        "mask_dir = gt_files[\"ShanghaiTech\"]\n",
        "mask_files = sorted(os.listdir(mask_dir))\n",
        "\n",
        "actual_labels = []\n",
        "for mask_file in mask_files:\n",
        "    mask_path = os.path.join(mask_dir, mask_file)\n",
        "    mask = scipy.io.loadmat(mask_path)[\"volLabel\"]\n",
        "    actual_labels.append(1 if np.sum(mask) > 0 else 0)\n",
        "\n",
        "actual_labels = np.array(actual_labels)\n",
        "gt_data_dict[\"ShanghaiTech\"] = actual_labels  # Store labels\n",
        "\n",
        "# Save to an Excel file with separate sheets\n",
        "file_path = \"ground_truth_labels.xlsx\"\n",
        "\n",
        "with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
        "    for dataset, labels in gt_data_dict.items():\n",
        "        df_gt = pd.DataFrame({\"Frame Index\": np.arange(len(labels)), \"Ground Truth\": labels})\n",
        "        df_gt.to_excel(writer, sheet_name=dataset, index=False)\n",
        "\n",
        "print(f\"Ground truth labels saved in '{file_path}' with sheets: {list(gt_data_dict.keys())}\")\n"
      ],
      "metadata": {
        "id": "ig0dyH5TvpS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load predicted frame scores\n",
        "df_pred = pd.read_excel(\"frame_scores_test.xlsx\")\n",
        "Predicted = df_pred[\"Frame Score\"].values\n",
        "\n",
        "# Load ground truth labels\n",
        "df_gt = pd.read_excel(\"ground_truth_labels.xlsx\", sheet_name=\"Avenue_GT\")  # Change sheet_name for different datasets\n",
        "Actual = df_gt[\"Ground Truth\"].values\n",
        "\n",
        "# Ensure Actual and Predicted are of the same length\n",
        "min_len = min(len(Actual), len(Predicted))\n",
        "Actual = Actual[:min_len]\n",
        "Predicted = Predicted[:min_len]\n",
        "\n",
        "# Compute ROC Curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(Actual, Predicted)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "# Determine optimal threshold (You can use a different method if needed)\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "# Apply threshold to get binary predictions\n",
        "y_pred = (Predicted >= optimal_threshold).astype(int)\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "cm = confusion_matrix(Actual, y_pred)\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(Actual, y_pred)\n",
        "recall = recall_score(Actual, y_pred)\n",
        "f1 = f1_score(Actual, y_pred)\n",
        "specificity = TN / (TN + FP)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"True Positives (TP): {TP}\")\n",
        "print(f\"True Negatives (TN): {TN}\")\n",
        "print(f\"False Positives (FP): {FP}\")\n",
        "print(f\"False Negatives (FN): {FN}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")\n",
        "print(f\"AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Purples\", cbar=True,\n",
        "                 xticklabels=['Normal (0)', 'Anomalous (1)'],\n",
        "                 yticklabels=['Normal (0)', 'Anomalous (1)'])\n",
        "plt.xlabel('Predicted Event', color='black')\n",
        "plt.ylabel('Actual Event', color='black')\n",
        "ax.tick_params(axis='both', colors='black')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, linestyle='--', marker='o', color='darkorange', lw=2, label=f'AUC = {auc_score:.4f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9KVuF0bAyKDS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}